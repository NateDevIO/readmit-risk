{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Risk Modeling: 30-Day Readmission Prediction\n",
    "\n",
    "**Objective**: Build a predictive model and generate risk scores for each patient.\n",
    "\n",
    "This notebook trains a machine learning model to predict 30-day hospital readmission risk, handles class imbalance with SMOTE, and exports risk scores for the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, confusion_matrix, \n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up paths\n",
    "DATA_DIR = Path('../data/raw')\n",
    "OUTPUT_DIR = Path('../data/processed')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"All libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and Initial Clean\n",
    "df = pd.read_csv(DATA_DIR / 'diabetic_data.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Replace '?' with NaN for proper handling\n",
    "df = df.replace('?', np.nan)\n",
    "\n",
    "# Create binary target FIRST (before any filtering)\n",
    "# 1 = readmitted within 30 days, 0 = not readmitted within 30 days\n",
    "df['readmitted_30day'] = (df['readmitted'] == '<30').astype(int)\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['readmitted_30day'].value_counts())\n",
    "print(f\"\\n30-day readmission rate: {df['readmitted_30day'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Handle Duplicate Patients\n",
    "# CRITICAL: Keep patient_nbr for deduplication, drop AFTER\n",
    "\n",
    "print(f\"\\nUnique patients: {df['patient_nbr'].nunique():,}\")\n",
    "print(f\"Total encounters: {len(df):,}\")\n",
    "print(f\"Average encounters per patient: {len(df)/df['patient_nbr'].nunique():.2f}\")\n",
    "\n",
    "# Keep only the FIRST encounter for each patient to avoid data leakage\n",
    "# (A patient's second visit would have information about their first readmission)\n",
    "df = df.sort_values('encounter_id').drop_duplicates(subset=['patient_nbr'], keep='first')\n",
    "print(f\"\\nAfter keeping first encounter per patient: {len(df):,} rows\")\n",
    "\n",
    "# Store patient IDs for later reference before dropping\n",
    "patient_ids = df['patient_nbr'].values.copy()\n",
    "\n",
    "# NOW we can drop patient identifiers (after deduplication)\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Engineering\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 4a: Handle columns with too many missing values (>40%)\n",
    "high_missing_cols = ['weight', 'payer_code', 'medical_specialty']\n",
    "print(f\"\\nDropping high-missing columns: {high_missing_cols}\")\n",
    "df = df.drop(columns=high_missing_cols)\n",
    "\n",
    "# 4b: Encode age ranges to numeric midpoints\n",
    "age_mapping = {\n",
    "    '[0-10)': 5, '[10-20)': 15, '[20-30)': 25, '[30-40)': 35,\n",
    "    '[40-50)': 45, '[50-60)': 55, '[60-70)': 65, '[70-80)': 75,\n",
    "    '[80-90)': 85, '[90-100)': 95\n",
    "}\n",
    "df['age_numeric'] = df['age'].map(age_mapping)\n",
    "print(f\"Encoded age to numeric (n={df['age_numeric'].notna().sum():,})\")\n",
    "\n",
    "# 4c: Create summary features\n",
    "df['total_visits'] = df['number_outpatient'] + df['number_emergency'] + df['number_inpatient']\n",
    "df['medication_intensity'] = df['num_medications'] / (df['time_in_hospital'] + 1)\n",
    "print(\"Created derived features: total_visits, medication_intensity\")\n",
    "\n",
    "# 4d: Medication change indicator (any medication changed during stay)\n",
    "med_cols = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', \n",
    "            'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', \n",
    "            'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', \n",
    "            'miglitol', 'troglitazone', 'tolazamide', 'insulin', \n",
    "            'glyburide-metformin', 'glipizide-metformin']\n",
    "\n",
    "def count_med_changes(row):\n",
    "    changes = 0\n",
    "    for col in med_cols:\n",
    "        if col in row.index and row[col] in ['Up', 'Down']:\n",
    "            changes += 1\n",
    "    return changes\n",
    "\n",
    "df['num_med_changes'] = df.apply(count_med_changes, axis=1)\n",
    "print(f\"Created num_med_changes feature\")\n",
    "\n",
    "# 4e: A1C result indicator\n",
    "df['A1Cresult_abnormal'] = df['A1Cresult'].apply(\n",
    "    lambda x: 1 if x in ['>7', '>8'] else 0\n",
    ")\n",
    "print(\"Created A1Cresult_abnormal feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Select and Encode Features\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE SELECTION AND ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define feature groups\n",
    "numeric_features = [\n",
    "    'time_in_hospital', 'num_lab_procedures', 'num_procedures',\n",
    "    'num_medications', 'number_outpatient', 'number_emergency',\n",
    "    'number_inpatient', 'number_diagnoses', 'age_numeric',\n",
    "    'total_visits', 'medication_intensity', 'num_med_changes'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'race', 'gender', 'admission_type_id', 'discharge_disposition_id',\n",
    "    'admission_source_id', 'diabetesMed', 'change', 'A1Cresult_abnormal'\n",
    "]\n",
    "\n",
    "# Keep only selected features plus target\n",
    "keep_cols = numeric_features + categorical_features + ['readmitted_30day']\n",
    "df_model = df[keep_cols].copy()\n",
    "\n",
    "print(f\"Selected {len(numeric_features)} numeric features\")\n",
    "print(f\"Selected {len(categorical_features)} categorical features\")\n",
    "\n",
    "# Handle any remaining missing values in numeric features\n",
    "for col in numeric_features:\n",
    "    if df_model[col].isnull().any():\n",
    "        median_val = df_model[col].median()\n",
    "        df_model[col] = df_model[col].fillna(median_val)\n",
    "        print(f\"  Filled {col} missing with median: {median_val:.2f}\")\n",
    "\n",
    "# Handle missing in categorical features\n",
    "for col in categorical_features:\n",
    "    if df_model[col].isnull().any():\n",
    "        df_model[col] = df_model[col].fillna('Unknown')\n",
    "        print(f\"  Filled {col} missing with 'Unknown'\")\n",
    "\n",
    "# One-hot encode categorical features\n",
    "df_encoded = pd.get_dummies(df_model, columns=categorical_features, drop_first=True)\n",
    "print(f\"\\nAfter encoding: {df_encoded.shape[1]} total features\")\n",
    "\n",
    "# Prepare X and y\n",
    "feature_cols = [col for col in df_encoded.columns if col != 'readmitted_30day']\n",
    "X = df_encoded[feature_cols]\n",
    "y = df_encoded['readmitted_30day']\n",
    "\n",
    "print(f\"\\nFinal X shape: {X.shape}\")\n",
    "print(f\"Final y shape: {y.shape}\")\n",
    "print(f\"Class balance: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train-Test Split with Class Imbalance Handling\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HANDLING CLASS IMBALANCE WITH SMOTE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split BEFORE applying SMOTE (to avoid data leakage)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"\\nBefore SMOTE - Training class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\nAfter SMOTE - Training class distribution:\")\n",
    "print(pd.Series(y_train_balanced).value_counts())\n",
    "print(f\"\\nBalanced training set size: {X_train_balanced.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model Training and Evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scale features for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression (interpretable, good for healthcare)\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, C=0.1)\n",
    "lr_model.fit(X_train_scaled, y_train_balanced)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "y_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Average Precision Score: {avg_precision:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualize Model Performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# ROC Curve\n",
    "ax1 = axes[0]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "ax1.plot(fpr, tpr, color='#3498db', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "ax1.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve', fontweight='bold')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "ax2 = axes[1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "ax2.plot(recall, precision, color='#e74c3c', lw=2, label=f'AP = {avg_precision:.3f}')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curve', fontweight='bold')\n",
    "ax2.legend(loc='lower left')\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "ax3 = axes[2]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3,\n",
    "            xticklabels=['Not Readmitted', 'Readmitted'],\n",
    "            yticklabels=['Not Readmitted', 'Readmitted'])\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Actual')\n",
    "ax3.set_title('Confusion Matrix', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'model_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Feature Importance Analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get coefficients from logistic regression\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': lr_model.coef_[0]\n",
    "}).sort_values('coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 RISK FACTORS (increase readmission risk):\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nTop 10 PROTECTIVE FACTORS (decrease readmission risk):\")\n",
    "print(feature_importance.tail(10).to_string(index=False))\n",
    "\n",
    "# Visualize top features\n",
    "top_features = pd.concat([\n",
    "    feature_importance.head(10),\n",
    "    feature_importance.tail(10)\n",
    "]).sort_values('coefficient')\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "colors = ['#e74c3c' if x > 0 else '#27ae60' for x in top_features['coefficient']]\n",
    "plt.barh(top_features['feature'], top_features['coefficient'], color=colors)\n",
    "plt.xlabel('Coefficient (Impact on Readmission Risk)')\n",
    "plt.title('Top Risk & Protective Factors for 30-Day Readmission', fontweight='bold')\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Generate Risk Scores and Export JSON\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING RISK SCORES AND EXPORTING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Score ALL patients (using unbalanced original data)\n",
    "X_all_scaled = scaler.transform(X)\n",
    "all_risk_scores = lr_model.predict_proba(X_all_scaled)[:, 1] * 100  # Convert to percentage\n",
    "\n",
    "# Add risk scores to original dataframe\n",
    "df_scored = df_model.copy()\n",
    "df_scored['risk_score'] = all_risk_scores\n",
    "\n",
    "# Calculate estimated cost per readmission\n",
    "AVG_READMISSION_COST = 15000\n",
    "df_scored['estimated_cost'] = (df_scored['risk_score'] / 100) * AVG_READMISSION_COST\n",
    "\n",
    "# Add patient ID for reference\n",
    "df_scored['patient_id'] = range(1, len(df_scored) + 1)\n",
    "\n",
    "# Select top 1000 highest risk patients for dashboard (performance optimization)\n",
    "df_high_risk = df_scored.nlargest(1000, 'risk_score')\n",
    "\n",
    "print(f\"Total patients scored: {len(df_scored):,}\")\n",
    "print(f\"High-risk patients exported: {len(df_high_risk):,}\")\n",
    "print(f\"Risk score range: {df_scored['risk_score'].min():.1f}% - {df_scored['risk_score'].max():.1f}%\")\n",
    "print(f\"Total estimated cost exposure: ${df_high_risk['estimated_cost'].sum():,.0f}\")\n",
    "\n",
    "# Prepare export data with clean column names\n",
    "export_columns = [\n",
    "    'patient_id', 'age_numeric', 'time_in_hospital', 'num_medications',\n",
    "    'number_diagnoses', 'number_inpatient', 'number_emergency',\n",
    "    'total_visits', 'num_med_changes', 'risk_score', 'estimated_cost',\n",
    "    'readmitted_30day'\n",
    "]\n",
    "\n",
    "# Rename age_numeric to age for cleaner export\n",
    "export_df = df_high_risk[export_columns].copy()\n",
    "export_df = export_df.rename(columns={'age_numeric': 'age'})\n",
    "\n",
    "# Round numeric values for cleaner JSON\n",
    "export_df['risk_score'] = export_df['risk_score'].round(2)\n",
    "export_df['estimated_cost'] = export_df['estimated_cost'].round(2)\n",
    "export_df['total_visits'] = export_df['total_visits'].astype(int)\n",
    "export_df['medication_intensity'] = (df_high_risk['medication_intensity']).round(2)\n",
    "\n",
    "# Convert to list of dictionaries for JSON\n",
    "patient_risks = export_df.to_dict('records')\n",
    "\n",
    "# Export to JSON\n",
    "with open(OUTPUT_DIR / 'patient_risks.json', 'w') as f:\n",
    "    json.dump(patient_risks, f, indent=2)\n",
    "\n",
    "print(f\"\\nExported patient_risks.json ({len(patient_risks)} records)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Generate Summary Statistics for Dashboard\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING SUMMARY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Risk distribution by bins\n",
    "bins = [0, 20, 40, 60, 80, 100]\n",
    "labels = ['0-20%', '20-40%', '40-60%', '60-80%', '80-100%']\n",
    "df_scored['risk_bin'] = pd.cut(df_scored['risk_score'], bins=bins, labels=labels)\n",
    "risk_distribution = df_scored['risk_bin'].value_counts().sort_index().to_dict()\n",
    "\n",
    "# Age group analysis\n",
    "age_bins = [0, 30, 50, 70, 100]\n",
    "age_labels = ['Under 30', '30-49', '50-69', '70+']\n",
    "df_scored['age_group'] = pd.cut(df_scored['age_numeric'], bins=age_bins, labels=age_labels)\n",
    "age_risk = df_scored.groupby('age_group', observed=True)['risk_score'].mean().to_dict()\n",
    "\n",
    "# Summary object\n",
    "risk_summary = {\n",
    "    'total_patients': int(len(df_scored)),\n",
    "    'high_risk_count': int(len(df_high_risk)),\n",
    "    'total_cost_exposure': float(df_high_risk['estimated_cost'].sum()),\n",
    "    'avg_risk_score': float(df_high_risk['risk_score'].mean()),\n",
    "    'median_risk_score': float(df_high_risk['risk_score'].median()),\n",
    "    'risk_distribution': {k: int(v) for k, v in risk_distribution.items()},\n",
    "    'avg_risk_by_age': {k: round(float(v), 2) for k, v in age_risk.items()},\n",
    "    'model_auc': float(roc_auc),\n",
    "    'readmission_rate_overall': float(df_scored['readmitted_30day'].mean() * 100)\n",
    "}\n",
    "\n",
    "# Export summary\n",
    "with open(OUTPUT_DIR / 'risk_summary.json', 'w') as f:\n",
    "    json.dump(risk_summary, f, indent=2)\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "for key, value in risk_summary.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"    {k}: {v}\")\n",
    "    elif isinstance(value, float):\n",
    "        print(f\"  {key}: {value:,.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "\n",
    "print(f\"\\nExported risk_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Model Summary and Next Steps\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = f\"\"\"\n",
    "MODEL: Logistic Regression with SMOTE oversampling\n",
    "\n",
    "PERFORMANCE METRICS:\n",
    "- ROC-AUC: {roc_auc:.4f}\n",
    "- Average Precision: {avg_precision:.4f}\n",
    "\n",
    "TOP RISK FACTORS:\n",
    "1. {feature_importance.head(1)['feature'].values[0]}\n",
    "2. {feature_importance.head(2).tail(1)['feature'].values[0]}\n",
    "3. {feature_importance.head(3).tail(1)['feature'].values[0]}\n",
    "\n",
    "EXPORTED FILES:\n",
    "- patient_risks.json: {len(patient_risks)} high-risk patient records\n",
    "- risk_summary.json: Dashboard summary statistics\n",
    "- model_performance.png: ROC, PR curves, confusion matrix\n",
    "- feature_importance.png: Top risk/protective factors\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Run 03_hospital_analytics.ipynb for geographic analysis\n",
    "2. Copy JSON files to dashboard/lib/ directory\n",
    "3. Build Next.js dashboard\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
